{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/meg_logo.png\" width=\"250px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAMP: predicting source of MEG signal\n",
    "<br>\n",
    "<div style=\"text-align: center\">\n",
    "    <em>\n",
    "        <i>Authors: Maria Teleńczuk, Lucy Liu, Hicham Janati, Guillaume Lemaitre, Alexandre Gramfort</i><br>\n",
    "        <a href=\"http://www.datascience-paris-saclay.fr\">Paris Saclay Center for Data Science</a> (Inria)\n",
    "    </em>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "1. [Introduction](#Introduction)\n",
    "    - [Origin of electrical signal in the brain](#EEG)\n",
    "    - [Origin of magnetic signal in the brain](#MEG)\n",
    "    - [MEG in practice and problem description](#MEG_in_practice)\n",
    "2. [Data exploration](#Data_exploration)\n",
    "    - [Import Python libraries](#Import)\n",
    "    - [Download the data](#Download_data)\n",
    "    - [MEG recordings](#X) \n",
    "    - [K-nearest neighbors algorithm](#KNN)\n",
    "    - [Use of lead fields](#lars)\n",
    "    - [Lasso Lars algorithm](#lassolars)\n",
    "3. [Submission](#Submission) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a class=\"anchor\" id=\"Introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brain activity produces electrical currents which are the origin of the magnetic field. Both electric and magnetic signals can be recorded from the scalp of the subject with help of electroencephalography (EEG) and magnetoencephalography (MEG). \n",
    "Here, we will focus on the magnetic signals recorded by MEG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origin of electrical signal in the brain  <a class=\"anchor\" id=\"EEG\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communication between brain cells (neurons) happens at the locations called synapses. That's where the signal passed from one neuron to another causes the electrical current to flow within and outside. A current flowing into one part of the cell (forming a current sink), flows within the cell and must leave it elsewhere (forming a current source). The pair of source and sink forms a current dipole. The dipole generated by a single cell can be therefore understood as a vector with constantly changing direction and length. Below you can see the visualisation of the positive current (excitator synaptic input) comming at different locations of the neuron. Please note, that in real conditions there are many such inputs happening simultaneusly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "     .equalDivide tr td { width:25%; }\n",
    "</style>\n",
    "\n",
    "<table class=\"equalDivide\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\" border=\"0\">\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "            <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/4neurons.gif\" width=\"400px\" ALIGN=”left”>\n",
    "        </td>\n",
    "    <td width=\"50%\">\n",
    "        <b>A schema of 4 neurons with the same morphology.</b> Each is stimulated with the synaptic input at different location (<span style=\"color: #FF0000\">red dot</span>) and at the same time. This is where the current enters the cell. Then, most of it flows through the cell and comes out in the cell body (<span style=\"color: #4B0082\">purple dot</span>) forming a dipole. The direction and relative size of the dipole of each cell is represented by a <span style=\"color: #4B0082\">purple line</span>. Note the difference. The colorful background shows the changes in the extracellular field.\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons are constantly active, constantly receiving and propagating the electrical signal, but a single neuron is relatively tiny and therefore the potential it generates is too small to be recorded from the scalp. However, there are billions of neurons in the human brain which together form brain structures. Many of the neuron types align and correlate in the activity. As you can imagine, in this environment some of the single-cell dipoles are cancelled out while the others add up to form much stronger signal. Now, this group signal along with a lot of noise can be recorded by EEG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origin of magnetic signal in the brain  <a class=\"anchor\" id=\"MEG\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only spoken of the electric currents, but you might remember from your physics class that electric currents are always associated with magnetic field. Now, if you consider the electric currents and the dipoles which we discussed above you can imagine magnetic fields forming closed loops  around them. Note that due to alignment of cell bodies in the brain, the magnetic fields are generated by the intracellular current (i.e. current flowing inside the cell) rather than the transmembrane currents (flowing inside/outside the cell) that are responsible for EEG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "     .equalDivide tr td { width:25%; }\n",
    "</style>\n",
    "\n",
    "<table class=\"equalDivide\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\" border=\"0\">\n",
    "    <tr>\n",
    "    <td width=\"30%\">\n",
    "         <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/magnetic_schema_small.png\" width=\"250px\" ALIGN=”left” /> \n",
    "    </td>\n",
    "    <td width=\"30%\">\n",
    "        The current flow is represented by the purple line (left), and the red lines show the direction of the magnetic field. Those magnetic fields are then recorded by the MEG sensors (grey on the right).\n",
    "    </td>\n",
    "    <td width=\"50%\">\n",
    "        <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/meg_scetch.png\" width=\"250px\" ALIGN=”left” /> \n",
    "    </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this is only a simplified explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEG in practice and problem description<a class=\"anchor\" id=\"MEG_in_practice\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of things happening in the human brain at every moment, so how is it possible to know what to look for? The subject participating in a cognitive neuroscience experiment is usually asked to perform the same task multiple times (watching something on a screen, remember something, pressing buttons etc.). Then the recorded signals obtained during all the repetitions of the experiment are averaged out leading to noise removal and clearer data related to that task. This is related to so-called [evoked responses](https://en.wikipedia.org/wiki/Evoked_potential). However, there are other challenges facing the data analyst: although MEG has many sensors measuring magnetic field around the scalp it is difficult to judge where exactly the signal is coming from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the question we ask you in this challenge: given some simulated MEG data you should predict the brain region(s) (sources) which are at the origin of the signals. Let's explore the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration <a class=\"anchor\" id=\"Data_exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import <a class=\"anchor\" id=\"Import\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python >= 3.7\n",
    "- [numpy](https://pypi.org/project/numpy/)\n",
    "- [scipy](https://pypi.org/project/scipy/)\n",
    "- [pandas](https://pypi.org/project/pandas/)\n",
    "- [scikit-learn](https://pypi.org/project/scikit-learn/)\n",
    "- [matplolib](https://pypi.org/project/matplotlib/)\n",
    "- [jupyter](https://pypi.org/project/jupyter/)\n",
    "- [ramp-workflow](https://pypi.org/project/ramp-workflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will install if necessary the missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from scikit-learn) (1.18.1)\n",
      "Collecting https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master\n",
      "  Using cached https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master\n",
      "Requirement already satisfied (use --upgrade to upgrade): ramp-workflow==0.4.0.dev0 from https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages\n",
      "Requirement already satisfied: numpy in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow==0.4.0.dev0) (1.18.1)\n",
      "Requirement already satisfied: scipy in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow==0.4.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: pandas in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow==0.4.0.dev0) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow==0.4.0.dev0) (0.22.1)\n",
      "Requirement already satisfied: joblib in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow==0.4.0.dev0) (0.14.1)\n",
      "Requirement already satisfied: cloudpickle in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow==0.4.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: click in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow==0.4.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from pandas->ramp-workflow==0.4.0.dev0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from pandas->ramp-workflow==0.4.0.dev0) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas->ramp-workflow==0.4.0.dev0) (1.14.0)\n",
      "Building wheels for collected packages: ramp-workflow\n",
      "  Building wheel for ramp-workflow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ramp-workflow: filename=ramp_workflow-0.4.0.dev0-py3-none-any.whl size=124968 sha256=582f7f54babaa83bbac64be02a62fbe810edd6247ed791f1da85c539fc9f8d89\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ocph721h/wheels/b7/18/af/cba50ad54ec8862831140e9f4fa3d006795fba0adab31d2853\n",
      "Successfully built ramp-workflow\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "\n",
    "# Install ramp-workflow from the master branch on GitHub.\n",
    "!{sys.executable} -m pip install https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required dependencies and downloads\n",
    "Installation of libraries and ramp-workflow\n",
    "\n",
    "To get this notebook running and test your models locally using the `ramp_test_submission`, we recommend that you use the Python distribution from Anaconda or Miniconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data (optional) <a class=\"anchor\" id=\"Download_data\"></a>\n",
    "\n",
    "If the data has not yet been downloaded locally, uncomment the following cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python download_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to find the `test` and `train` folders in the `data/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEG recordings <a class=\"anchor\" id=\"X\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10',\n",
       "       ...\n",
       "       'e196', 'e197', 'e198', 'e199', 'e200', 'e201', 'e202', 'e203', 'e204',\n",
       "       'subject'],\n",
       "      dtype='object', length=205)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv(\"data/train/X.csv.gz\")\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has a lot of columns named e1, e2, .. ,e204 and a column named 'subject'. Each column marked with 'e' is a recording from one of the MEG sensors. There are 204 sensors in this MEG recordings. 'subject' is the subject id on whom this recording was performed. Let's see what subjects do we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['subject_1', 'subject_2', 'subject_3', 'subject_4', 'subject_5'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 205)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the heat maps of the first three samples on the head:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/topomaps.png\" width=\"500px\" ALIGN=”left” /> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) If you wish to see and run the code which plots the above heatmaps, you will have to additionally install [MNE](https://pypi.org/project/mne/) library and uncomment the following line and then run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load plot_topomap.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above heat maps are taken from the first three samples of the `train` dataset. The darker the color, the higher is the recorded value. Can you already make a guess how many sources lead to generation of this signals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at the ground truth let's discuss what do we actually mean by 'source'. The brain is a continous mass and so we could consider millions of points to be a potential source. However, for the sake of this study we subdivided the brain of each subject to 450 regions (225 subregions per hemisphere,  <i>corpus callosum</i> located between the two hemispheres excluded). Each of the subregions is a part of a larger region which has an anatomical meaning (represented by different colors below):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/aparc_brain.png\" width=\"500px\" ALIGN=”left” />  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to predict in which subregion(s) the MEG signal originates from.\n",
    "So let's look at the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sparse.load_npz('data/train/target.npz').toarray()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can see is an array mostly filled with 0s. Each column represent a different region. 1s represent the sources. You should be able to guess what is the shape of the target, can you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2500 samples and 450 brain regions\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {y.shape[0]} samples and {y.shape[1]} brain regions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if you guessed correctly the number of sources in each of the three heatmaps above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sources in first three samples: [2. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "n_sources = np.sum(y, axis=1)\n",
    "print(f'Number of sources in first three samples: {n_sources[0:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we simulated this data (using [MNE](https://mne.tools/stable/index.html) Python library) we were free to limit the number of sources. Let's check what are the number of sources in other samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible number of sources: [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "n_sources_per_sample = np.sum(y, axis=1)\n",
    "n_sources = np.unique(n_sources_per_sample)\n",
    "print(f'Possible number of sources: {n_sources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest neighbors algorithm <a class=\"anchor\" id=\"KNN\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now goint to make some predictions. We will start from the algorithm called k-nearest neighbors. You can read more about it in the [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)  or [Scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). For reading and writing the data we will now use functions stored in `problem.py`. The same functions will be used by RAMP when scoring your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2500 measurements recorded from 5 subjects in the train dataset, and\n",
      "2500 measurements recorded from 5 subjects in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "# loading the data\n",
    "from problem import get_train_data, get_test_data\n",
    "\n",
    "X_train, y_train = get_train_data()\n",
    "X_test, y_test = get_test_data()\n",
    "\n",
    "# print info\n",
    "print(f\"There are {len(X_train)} measurements\"\n",
    "      f\" recorded from {len(np.unique(X_train['subject']))} subjects\"\n",
    "      \" in the train dataset, and\\n\"\n",
    "      f\"{len(X_test)} measurements\"\n",
    "      f\" recorded from {len(np.unique(X_test['subject']))} subjects\"\n",
    "      \" in the test dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all the libraries which we will need to write this estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply KNeigbors Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neihbors\n",
    "clf = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just use KNeigborsClassifier on our data it will not work because the target is multioutput meaning that we might have more than a single predicted output. That is why we also use [MultiOutputClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kneighbours = MultiOutputClassifier(clf, n_jobs=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time when you write the solution for RAMP you will have to pass it as a sklearn [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). However, what we have done so far is not yet sufficient. Because the data X consists not only of the sensor measurements (`dtype`: `float64`) but also of the subject id which is of `dtype`: `object`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e1         float64\n",
       "e2         float64\n",
       "e3         float64\n",
       "e4         float64\n",
       "e5         float64\n",
       "            ...   \n",
       "e201       float64\n",
       "e202       float64\n",
       "e203       float64\n",
       "e204       float64\n",
       "subject     object\n",
       "Length: 205, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighbors won't accept it in this form. Here, we decide to just drop the whole column and do not use the information about the subjects. We can do it using [ColumnTranformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) or function [make_column_transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer((\"drop\", 'subject'),\n",
    "                                       remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply the Scikit-learn pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('transformer', preprocessor),\n",
    "        ('classifier', clf)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code presented above is implemented as a sample solution in: `submissions/starting_kit/estimator.py`. If you wish to load it here, uncomment the line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load submissions/starting_kit/estimator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit this pipeline with the data and make a prediction. We will then use [hamming loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html) to make the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_knn = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hamming loss for KNN is 0.004828444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "score = hamming_loss(y_test, y_pred_knn)\n",
    "print(f\"The hamming loss for KNN is {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted number of sources: [0. 1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "n_sources_per_sample = np.sum(y_pred_knn, axis = 1)\n",
    "n_sources = np.unique(n_sources_per_sample)\n",
    "print(f'Predicted number of sources: {n_sources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lead fields <a class=\"anchor\" id=\"lars\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at your data files you probably realized that there are more than only `X.csv` and `target.npz` in both `data/train` and `data/test` folders but also you have some files stored in `data/` directory. Their names begin with some `id` and end with \"_lead_field.npz\". Perhaps you have noticed that the `id` corresponds to the `id` of the `subject` in your `X.csv` data file. Let's make sure that we have the same number of subjects in the data as provided lead_field files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 lead field files and 10 subjects\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "data_dir = 'data/'\n",
    "\n",
    "lead_field_files = os.path.join(data_dir, '*lead_field.npz')\n",
    "lead_field_files = sorted(glob.glob(lead_field_files))\n",
    "n_subj_train = np.unique(X_train['subject'])\n",
    "n_subj_test = np.unique(X_test['subject'])\n",
    "len_unique = (len(n_subj_train) +\n",
    "              len(n_subj_test) -\n",
    "              len(np.intersect1d(n_subj_train, n_subj_test)))\n",
    "\n",
    "print(f\"{len(lead_field_files)} lead field files and\"\n",
    "      f\" {len_unique} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each brain is different in a shape and a structure. Therefore the signal from the source to the sensors propagates differently in each subject. You might think of those lead_fields as of weights between the sources and the sensors. Let's look at the shape of one of those files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 4690)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = np.load(lead_field_files[0])\n",
    "L['lead_field'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the shape of a Lead Field that you might have expected. 204 is the number of sensors. But why number of regions is not 450? As we mentioned previously each region we consider is of a specific size, but the source can by any single point within this region and lead_field stores the value corresponding to every of those points. Furthermore, this number of points will differ between subjects. Let's look at the lead_field of another subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 4688)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = np.load(lead_field_files[1])\n",
    "L['lead_field'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we match which point belongs to which region? In your `lead_field` file you will find another argument called `parcel_indices`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4688 consisting of 450 numbers\n"
     ]
    }
   ],
   "source": [
    "parcel_indices = L['parcel_indices']\n",
    "print(f\"There are {len(parcel_indices)} consisting of {len(np.unique(parcel_indices))} numbers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning that each number in parcel_indices tells us which which point of the `lead_field` belongs to which region of the `target`. How can we use this information for the predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Lars algorithm <a class=\"anchor\" id=\"lassolars\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct slightly more complicated estimator which will use lead fields. First we want to load those lead_fields which are used in our data. Note that we are scaling all the lead_fields by 1e8. That is to avoid having too small numbers given to the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 lead_fields and 10 parcel_indices\n",
      "Created dictionary of subject_ids and matching indices: {'subject_10': 0, 'subject_1': 1, 'subject_2': 2, 'subject_3': 3, 'subject_4': 4, 'subject_5': 5, 'subject_6': 6, 'subject_7': 7, 'subject_8': 8, 'subject_9': 9}\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "data_dir = 'data/'\n",
    "\n",
    "# find all the files ending with '_lead_field' in the data directory\n",
    "lead_field_files = os.path.join(data_dir, '*lead_field.npz')\n",
    "lead_field_files = sorted(glob.glob(lead_field_files))\n",
    "\n",
    "parcel_indices_leadfield, L = [], []\n",
    "subj_dict = {}\n",
    "for idx, lead_file in enumerate(lead_field_files):\n",
    "    lead_matrix = np.load(lead_file)\n",
    "\n",
    "    lead_file = os.path.basename(lead_file)\n",
    "    subj_dict['subject_' + lead_file.split('_')[1]] = idx\n",
    "\n",
    "    parcel_indices_leadfield.append(lead_matrix['parcel_indices'])\n",
    "\n",
    "    # scale L to avoid tiny numbers\n",
    "    L.append(1e8 * lead_matrix['lead_field'])\n",
    "    assert parcel_indices_leadfield[idx].shape[0] == L[idx].shape[1]\n",
    "\n",
    "assert len(parcel_indices_leadfield) == len(L) == idx + 1\n",
    "assert len(subj_dict) >= 1  # at least a single subject\n",
    "\n",
    "print(f'Loaded {len(L)} lead_fields and {len(parcel_indices_leadfield)} parcel_indices')\n",
    "print(f'Created dictionary of subject_ids and matching indices: {subj_dict}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the `subj_dict` to keep track which row of `L` and which row of `parcel_indices_leadfield` correspond to which subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use `subj_dict` to map the subjects in the X datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mapped = X_train.copy()\n",
    "X_train_mapped['subject_id'] = X_train['subject'].map(subj_dict)\n",
    "# scale to avoid tiny numbers\n",
    "X_train_mapped.iloc[:, :-2] *= 1e12\n",
    "\n",
    "X_test_mapped = X_test.copy()\n",
    "X_test_mapped['subject_id'] = X_test_mapped['subject'].map(subj_dict)\n",
    "# scale to avoid tiny numbers\n",
    "X_test_mapped.iloc[:, :-2] *= 1e12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a class `SparseRegressor` which will accept the estimator (ie model) with which it will make the decision using lead fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "def _get_coef(est):\n",
    "    if hasattr(est, 'steps'):\n",
    "        return est.steps[-1][1].coef_\n",
    "    return est.coef_\n",
    "\n",
    "\n",
    "class SparseRegressor(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    def __init__(self, lead_field, parcel_indices, model, n_jobs=1):\n",
    "        self.parcel_indices = parcel_indices\n",
    "        self.lead_field = lead_field\n",
    "        self.model = model\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.decision_function(X) > 0).astype(int)\n",
    "\n",
    "    def _run_model(self, model, L, X, fraction_alpha=0.2):\n",
    "        norms = np.linalg.norm(L, axis=0)\n",
    "        L = L / norms[None, :]\n",
    "\n",
    "        est_coefs = np.empty((X.shape[0], L.shape[1]))\n",
    "        for idx, idx_used in enumerate(X.index.values):\n",
    "            x = X.iloc[idx].values\n",
    "            model.fit(L, x)\n",
    "            est_coef = np.abs(_get_coef(model))\n",
    "            est_coef /= norms\n",
    "            est_coefs[idx] = est_coef\n",
    "\n",
    "        return est_coefs.T\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        X = X.reset_index(drop=True)\n",
    "\n",
    "        n_parcels = max(max(s) for s in self.parcel_indices)\n",
    "        betas = np.empty((len(X), n_parcels))\n",
    "        for subj_idx in np.unique(X['subject_id']):\n",
    "            l_used = self.lead_field[subj_idx]\n",
    "\n",
    "            X_used = X[X['subject_id'] == subj_idx]\n",
    "            X_used = X_used.iloc[:, :-2]\n",
    "\n",
    "            est_coef = self._run_model(self.model, l_used, X_used)\n",
    "\n",
    "            beta = pd.DataFrame(\n",
    "                       np.abs(est_coef)\n",
    "                   ).groupby(\n",
    "                   self.parcel_indices[subj_idx]).max().transpose()\n",
    "            betas[X['subject_id'] == subj_idx] = np.array(beta)\n",
    "        return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model_lars = linear_model.LassoLars(alpha=1.0, max_iter=3,\n",
    "                                    normalize=False,\n",
    "                                    fit_intercept=False)\n",
    "\n",
    "lasso_lars = SparseRegressor(L, parcel_indices_leadfield, model_lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_lars.fit(X_train_mapped, y_train)\n",
    "y_pred_lassolars = lasso_lars.predict(X_test_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss for the Lasso Lars using lead fields is 0.0044\n"
     ]
    }
   ],
   "source": [
    "score = hamming_loss(y_test, y_pred_lassolars)\n",
    "print(f'Hamming loss for the Lasso Lars using lead fields is {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is very similar to the one we got for knn. Let's see if the number of sources is more or less predicted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted possible number of sources: [0]\n"
     ]
    }
   ],
   "source": [
    "n_sources_by_sample = np.sum(y_pred_lassolars, axis = 1)\n",
    "n_sources = np.unique(n_sources_by_sample)\n",
    "print(f'Predicted possible number of sources: {n_sources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in fact, the LassoLars with this settings predicted no sources at all.\n",
    "We are getting different results, but the hamming loss remains almost the same. That is because it only calculates the fraction of wrongly predicted labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we try to change the score, let's look at the LassoLars. We previously set `alpha` to 1.0. We will now try setting it in relation to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "def _get_coef(est):\n",
    "    if hasattr(est, 'steps'):\n",
    "        return est.steps[-1][1].coef_\n",
    "    return est.coef_\n",
    "\n",
    "\n",
    "class SparseRegressorAlpha(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    def __init__(self, lead_field, parcel_indices, model, n_jobs=1):\n",
    "        self.lead_field = lead_field\n",
    "        self.parcel_indices = parcel_indices\n",
    "        self.model = model\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.decision_function(X) > 0).astype(int)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        model = MultiOutputRegressor(self.model, n_jobs=self.n_jobs)\n",
    "        X = X.reset_index(drop=True)\n",
    "\n",
    "        betas = np.empty((len(X), 0)).tolist()\n",
    "        for subj_idx in np.unique(X['subject_id']):\n",
    "            l_used = self.lead_field[subj_idx]\n",
    "\n",
    "            X_used = X[X['subject_id'] == subj_idx]\n",
    "            X_used = X_used.iloc[:, :-2]\n",
    "\n",
    "            norms = l_used.std(axis=0)\n",
    "            l_used = l_used / norms[None, :]\n",
    "\n",
    "            alpha_max = abs(l_used.T.dot(X_used.T)).max() / len(l_used)\n",
    "            alpha = 0.2 * alpha_max\n",
    "            model.estimator.alpha = alpha\n",
    "            model.fit(l_used, X_used.T)  # cross validation done here\n",
    "\n",
    "            for idx, idx_used in enumerate(X_used.index.values):\n",
    "                est_coef = np.abs(_get_coef(model.estimators_[idx]))\n",
    "                est_coef /= norms\n",
    "                beta = pd.DataFrame(\n",
    "                        np.abs(est_coef)\n",
    "                        ).groupby(\n",
    "                        self.parcel_indices[subj_idx]).max().transpose()\n",
    "                betas[idx_used] = np.array(beta).ravel()\n",
    "        betas = np.array(betas)\n",
    "        return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lars_alpha = linear_model.LassoLars(max_iter=3,\n",
    "                                          normalize=False,\n",
    "                                          fit_intercept=False)\n",
    "\n",
    "lasso_lars_alpha = SparseRegressorAlpha(L, parcel_indices_leadfield,\n",
    "                                        model_lars_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_lars_alpha.fit(X_train_mapped, y_train)\n",
    "y_pred_alpha = lasso_lars_alpha.predict(X_test_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss for the Lasso Lars using lead fields is 0.004428444444444444\n"
     ]
    }
   ],
   "source": [
    "score = hamming_loss(y_test, y_pred_alpha)\n",
    "print(f'Hamming loss for the Lasso Lars using lead fields is {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible number of sources: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "n_sources_by_sample = np.sum(y_pred_alpha, axis = 1)\n",
    "n_sources = np.unique(n_sources_by_sample)\n",
    "print(f'Possible number of sources: {n_sources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the above algorithm in `RAMP` you need to change it to be able to return a `scikit-learn` type of pipeline. This is saved in the `submissions/lasso_lars/estimator.py`. You can load the code here by uncommenting the line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'submissions/lasso_lars/estimator.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our estimator is predicting more feasable number of sources at each sample. But the score still remains the same. Let's calculate all the three results with the jaccard error (meaning 1-[jaccard score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jaccard error for KNN model is 0.9219533333333333,\n",
      "for the model which predicts only 0s is 1.0,\n",
      "for SparseRegressor with LassoLars as a model and updating alpha is 0.6583266666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "score_knn = 1 - jaccard_score(y_test, y_pred_knn, average='samples')\n",
    "score_lassolars = 1 - jaccard_score(y_test, y_pred_lassolars, average='samples')\n",
    "score_alpha = 1 - jaccard_score(y_test, y_pred_alpha, average='samples')\n",
    "\n",
    "print(f'The Jaccard error for KNN model is {score_knn},')\n",
    "print(f'for the model which predicts only 0s is {score_lassolars},')\n",
    "print(f'for SparseRegressor with LassoLars as a model and updating alpha is {score_alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Jaccard error you can indeed see that the last model gave us the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission <a class=\"anchor\" id=\"Submission\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you found a good model you wish to test you should place it in the directory with the name of your choice and place it in the `submissions` folder (you can already find there two submissions named `starting_kit` and `lasso_lars` which we talked about above). The file placed in your submission directory should be called `estimator.py` and should return `scikit-learn` type of pipeline.\n",
    "\n",
    "You might then test your submission locally using command:\n",
    "\n",
    "`ramp-test --submission <your submission folder name>`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on how to submit your code on [ramp.studio](https://ramp.studio/), refer to the [online documentation](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/stable/using_kits.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
